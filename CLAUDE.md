# MBA Deobfuscator - ML System for Polynomial Expression Simplification

> **Mission**: Simplify obfuscated Mixed Boolean-Arithmetic expressions using GNN+Transformer architecture with formal verification.

**Status**: Production-ready codebase (85-90% complete) | **Parameters**: 15M (base) / 360M (scaled)

---

## Quick Start

```bash
# Generate training data
python scripts/generate_data.py --depth 1-14 --samples 10M

# Train Phase 1 (Contrastive Pretraining)
python scripts/train.py --phase 1 --config configs/phase1.yaml

# Train Phase 2 (Supervised Learning)
python scripts/train.py --phase 2 --config configs/phase2.yaml

# Train Phase 3 (RL Fine-Tuning)
python scripts/train.py --phase 3 --config configs/phase3.yaml

# Inference
python scripts/simplify.py --expr "(x&y)+(x^y)" --checkpoint best.pt
```

---

## Architecture Overview

```
Input Expression
    â†“
AST Parser â†’ Graph Construction
    â†“
[GNN Encoder] â†’ Node Embeddings
    â†“
[Semantic Fingerprint] â†’ 416-dim vector (448 raw - 32 derivatives)
    â†“
Fingerprint Fusion â†’ Combined representation
    â†“
[Transformer Decoder] â†’ Token sequence
    â†“
[Output Heads] â†’ Tokens + Complexity + Value
    â†“
Beam Search / HTPS â†’ Candidate generation
    â†“
3-Tier Verification â†’ Syntax â†’ Execution â†’ Z3
    â†“
Reranking â†’ Final simplified expression
```

**Core Components**:
- **8 Encoder Architectures**: GAT+JKNet, GGNN, HGT, RGCN, Semantic HGT, Transformer-only, Hybrid GREAT, GMN variants
- **Semantic Fingerprint**: 416 floats (symbolic + corner evals + random hash + truth table)
- **Transformer Decoder**: 6 layers, 8 heads, 512d with copy mechanism
- **3-Tier Verification**: Syntax (~10Âµs) â†’ Execution (~1ms) â†’ Z3 SMT (~100ms)

---

## Project Structure

```
mba-deobfuscator/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ constants.py              # All hyperparameters (single source of truth)
â”‚   â”œâ”€â”€ data/                     # Data pipeline
â”‚   â”‚   â”œâ”€â”€ ast_parser.py         # Expression â†’ Graph conversion
â”‚   â”‚   â”œâ”€â”€ dataset.py            # 5 dataset variants (Contrastive, Supervised, Scaled, GMN)
â”‚   â”‚   â”œâ”€â”€ fingerprint.py        # 448-dim semantic fingerprint (C++ accelerated)
â”‚   â”‚   â”œâ”€â”€ tokenizer.py          # 300-vocab expression tokenizer
â”‚   â”‚   â”œâ”€â”€ walsh_hadamard.py     # Walsh-Hadamard spectral features
â”‚   â”‚   â”œâ”€â”€ collate.py            # Batch collation for PyG graphs
â”‚   â”‚   â”œâ”€â”€ augmentation.py       # Variable permutation augmentation
â”‚   â”‚   â””â”€â”€ dag_features.py       # DAG positional encoding
â”‚   â”œâ”€â”€ models/                   # Neural architecture
â”‚   â”‚   â”œâ”€â”€ encoder.py            # 8 encoder implementations (1097 lines)
â”‚   â”‚   â”œâ”€â”€ encoder_base.py       # BaseEncoder interface
â”‚   â”‚   â”œâ”€â”€ encoder_registry.py   # Encoder factory (get_encoder)
â”‚   â”‚   â”œâ”€â”€ semantic_hgt.py       # Semantic HGT with property detection
â”‚   â”‚   â”œâ”€â”€ decoder.py            # Transformer decoder + copy mechanism
â”‚   â”‚   â”œâ”€â”€ full_model.py         # MBADeobfuscator (end-to-end model)
â”‚   â”‚   â”œâ”€â”€ heads.py              # Token/Complexity/Value prediction heads
â”‚   â”‚   â”œâ”€â”€ property_detector.py  # Algebraic property detection
â”‚   â”‚   â”œâ”€â”€ global_attention.py   # GraphGPS-style global attention
â”‚   â”‚   â”œâ”€â”€ path_encoding.py      # Path-based edge encoding
â”‚   â”‚   â”œâ”€â”€ operation_aware_aggregator.py  # Op-specific message passing
â”‚   â”‚   â””â”€â”€ gmn/                  # Graph Matching Network modules
â”‚   â”œâ”€â”€ training/                 # Training infrastructure
â”‚   â”‚   â”œâ”€â”€ base_trainer.py       # Base trainer (optimizer, scheduler, checkpoints)
â”‚   â”‚   â”œâ”€â”€ phase1_trainer.py     # Contrastive pretraining (InfoNCE + MaskLM)
â”‚   â”‚   â”œâ”€â”€ phase1b_gmn_trainer.py     # GMN training (frozen encoder)
â”‚   â”‚   â”œâ”€â”€ phase1c_gmn_trainer.py     # GMN end-to-end fine-tuning
â”‚   â”‚   â”œâ”€â”€ phase2_trainer.py     # Supervised learning + curriculum
â”‚   â”‚   â”œâ”€â”€ phase3_trainer.py     # RL fine-tuning (PPO)
â”‚   â”‚   â”œâ”€â”€ ablation_trainer.py   # Encoder comparison with stats
â”‚   â”‚   â”œâ”€â”€ losses.py             # All loss functions
â”‚   â”‚   â””â”€â”€ negative_sampler.py   # Hard negative mining
â”‚   â”œâ”€â”€ inference/                # Inference pipeline
â”‚   â”‚   â”œâ”€â”€ pipeline.py           # End-to-end InferencePipeline
â”‚   â”‚   â”œâ”€â”€ beam_search.py        # Diverse beam search (50 beams, 4 groups)
â”‚   â”‚   â”œâ”€â”€ htps.py               # HyperTree Proof Search (UCB, 6 tactics)
â”‚   â”‚   â”œâ”€â”€ verify.py             # ThreeTierVerifier
â”‚   â”‚   â”œâ”€â”€ grammar.py            # Grammar-constrained decoding
â”‚   â”‚   â””â”€â”€ rerank.py             # Multi-criteria reranking
â”‚   â””â”€â”€ utils/                    # Utilities
â”‚       â”œâ”€â”€ z3_interface.py       # Z3 SMT solver wrapper
â”‚       â”œâ”€â”€ expr_eval.py          # Safe expression evaluation
â”‚       â”œâ”€â”€ metrics.py            # Training/eval metrics
â”‚       â”œâ”€â”€ ablation_*.py         # Ablation study utilities
â”‚       â”œâ”€â”€ config.py             # YAML config loading
â”‚       â””â”€â”€ logging.py            # Training logging
â”œâ”€â”€ scripts/                      # Executable scripts
â”‚   â”œâ”€â”€ train.py                  # Main training orchestrator
â”‚   â”œâ”€â”€ generate_data.py          # Dataset generation
â”‚   â”œâ”€â”€ evaluate.py               # Model evaluation
â”‚   â”œâ”€â”€ simplify.py               # Inference interface
â”‚   â”œâ”€â”€ verify_model.py           # Checkpoint verification
â”‚   â”œâ”€â”€ verify_gcnii.py           # GCNII testing
â”‚   â””â”€â”€ validate_fingerprint_consistency.py  # Fingerprint validation
â”œâ”€â”€ tests/                        # 23 test files (~70% coverage)
â”œâ”€â”€ configs/                      # YAML configurations (8 files)
â””â”€â”€ docs/                         # Detailed documentation
    â”œâ”€â”€ ARCHITECTURE.md           # Model architecture details
    â”œâ”€â”€ DATA_PIPELINE.md          # Data processing pipeline
    â”œâ”€â”€ TRAINING.md               # Training phases and curriculum
    â”œâ”€â”€ INFERENCE.md              # Inference and verification
    â””â”€â”€ API_REFERENCE.md          # Public API documentation
```

---

## Encoder Architectures (All Fully Implemented)

| Encoder | Params (256d) | Edge Types | Best For | Status |
|---------|---------------|------------|----------|--------|
| **GAT+JKNet** | ~2.8M | None (homogeneous) | Depth â‰¤10, fast training | âœ… Production |
| **GGNN** | ~3.2M | 6 or 8 types | Depth 10+, iterative refinement | âœ… Production |
| **HGT** | ~60M (768d) | 8 types (required) | Scaled model, heterogeneous | âœ… Production |
| **RGCN** | ~60M (768d) | 8 types (required) | Alternative to HGT | âœ… Production |
| **Semantic HGT** | ~68M (768d) | 8 types + properties | Property detection + WHT | âœ… Production |
| **Transformer-only** | ~12M | None (sequence) | Ablation baseline | âœ… Production |
| **Hybrid GREAT** | ~25M | None (mixed attn) | Mixed architecture | âœ… Production |
| **HGT+GMN** | ~70M | 8 types + matching | Graph matching capability | âœ… Production |

**Edge Type Systems**:
- **Legacy (6-type)**: CHILD_LEFT, CHILD_RIGHT, PARENT, SIBLING_NEXT, SIBLING_PREV, SAME_VAR
- **Optimized (8-type)**: LEFT_OPERAND, RIGHT_OPERAND, UNARY_OPERAND (+ inverses + domain bridges)

**Usage**:
```python
from src.models.encoder_registry import get_encoder

# Base model (15M total params)
encoder = get_encoder('gat_jknet', hidden_dim=256, num_layers=4)

# Scaled model (360M total params)
encoder = get_encoder('hgt', hidden_dim=768, num_layers=12)
```

---

## Training Pipeline (3 Phases + Variants)

### Phase 1: Contrastive Pretraining
**Goal**: Learn semantic expression representations without labels

```yaml
Loss: InfoNCE (Ï„=0.07) + MaskLM (mask_ratio=0.15)
Data: ContrastiveDataset (positive pairs = equivalent expressions)
Epochs: 20
Output: Pretrained encoder weights
```

**Variants**:
- **Phase 1b**: Train GMN head with frozen encoder (graph matching)
- **Phase 1c**: End-to-end GMN fine-tuning

### Phase 2: Supervised Learning
**Goal**: Learn simplification with curriculum

```yaml
Loss: CrossEntropy + Complexity (0.1) + Copy (0.1)
Data: MBADataset with self-paced curriculum
Curriculum:
  - Stage 1: depth â‰¤2 (10 epochs, target 95%)
  - Stage 2: depth â‰¤5 (15 epochs, target 90%)
  - Stage 3: depth â‰¤10 (15 epochs, target 80%)
  - Stage 4: depth â‰¤14 (10 epochs, target 70%)
Epochs: 50 total
Output: Full trained model
```

### Phase 3: RL Fine-Tuning
**Goal**: Optimize for equivalence and simplicity via PPO

```yaml
Algorithm: PPO with entropy regularization
Rewards:
  - Equivalence (Z3 verified): +10.0
  - Simplification ratio: +2.0 Ã— ratio
  - Identity penalty: -5.0
  - Syntax error: -1.0
Tactics: 6 fixed simplification rules
Epochs: 10
Output: Fine-tuned model
```

**Commands**:
```bash
python scripts/train.py --phase 1 --config configs/phase1.yaml
python scripts/train.py --phase 2 --config configs/phase2.yaml
python scripts/train.py --phase 3 --config configs/phase3.yaml
```

---

## Semantic Fingerprint (416-dim for ML)

**Raw fingerprint**: 448 dimensions
**ML fingerprint**: 416 dimensions (derivatives excluded due to C++/Python evaluation differences)

| Component | Dims | Method | Values |
|-----------|------|--------|--------|
| Symbolic | 32 | Structural analysis | Node degrees, op counts, depth, variables |
| Corner | 256 | 4 widths Ã— 64 cases | Extreme value evaluation (0, 1, -1, max, min) |
| Random | 64 | 4 widths Ã— 16 inputs | Deterministic hash inputs |
| ~~Derivative~~ | ~~32~~ | ~~4 widths Ã— 8 orders~~ | **EXCLUDED** (C++/Python mismatch) |
| Truth Table | 64 | 2^6 for 6 vars | Boolean function evaluation |

**Bit widths**: 8, 16, 32, 64 (deterministic evaluation at all widths)

**C++ Acceleration**: 10Ã— speedup via `mba_fingerprint_cpp` (optional, graceful fallback to Python)

```python
from src.data.fingerprint import SemanticFingerprint

fp = SemanticFingerprint()
vector = fp.compute(expression)  # Returns 448-dim ndarray
ml_vector = vector[:352] + vector[384:]  # Strip derivatives â†’ 416-dim
```

---

## Inference Pipeline

### Strategy Selection
- **Shallow (depth < 10)**: Beam search (faster, good for simple expressions)
- **Deep (depth â‰¥ 10)**: HTPS (compositional, handles complex expressions)

### Beam Search Configuration
```yaml
Beam width: 50
Diversity groups: 4 (with diversity penalty)
Temperature: 0.7
Length normalization: Wu et al. 2016 formula
Grammar constraints: Enabled (prevents invalid syntax)
```

### HTPS (HyperTree Proof Search)
```yaml
Algorithm: UCB (Upper Confidence Bound)
Budget: 500 node expansions
Tactics: 6 fixed rules
  - Identity laws (x&x=x, x|0=x, ...)
  - MBA patterns (x&y + x^y = x|y, ...)
  - Constant folding
  - Distributive laws
  - De Morgan's laws
  - Algebraic simplification
Exploration constant: c = âˆš2
```

### 3-Tier Verification Cascade

```
All candidates
    â†“
[Tier 1: Syntax] Grammar validation (~10Âµs per candidate)
    â†“ (filters ~60%)
[Tier 2: Execution] 100 random tests Ã— 4 widths (~1ms per candidate)
    â†“ (filters ~35%)
[Tier 3: Z3 SMT] Formal verification (~100-1000ms, top-10 only)
    â†“
Verified equivalents
```

**Efficiency**: 95% of candidates filtered before expensive Z3 calls

### Reranking Criteria
1. Verification tier reached (Z3 > Execution > Syntax)
2. Model confidence (softmax probability)
3. Simplification ratio (original_size / simplified_size)
4. Depth reduction (original_depth - simplified_depth)

**Usage**:
```python
from src.inference.pipeline import InferencePipeline

pipeline = InferencePipeline(model, mode='beam')  # or mode='htps'
result = pipeline.simplify("(x&y)+(x^y)")
print(result.simplified)  # "x | y"
print(result.verification_tier)  # "z3"
```

---

## Data Format

### Input JSONL (Training Data)
```json
{"obfuscated": "(x & y) + (x ^ y)", "simplified": "x | y", "depth": 3}
{"obfuscated": "x ^ x", "simplified": "0", "depth": 1}
```

### Tokenizer Vocabulary (300 tokens)
```
Special tokens: [PAD]=0, [UNK]=1, [BOS]=2, [EOS]=3, [MASK]=4
Operators: &, |, ^, +, -, *, ~, neg  (tokens 5-12)
Parentheses: (, )  (tokens 13-14)
Variables: x0-x7  (tokens 15-22)
Constants: 0-255  (tokens 23-277)
Reserved: 278-299
```

**Expression example**: `(x0 & x1) + (x0 ^ x1)`
**Tokenized**: `[13, 15, 5, 16, 14, 6, 13, 15, 7, 16, 14]`

---

## Key Features & Novel Approaches

### Core Infrastructure

âœ… **ScaledMBADataset** (Default for All Training)
- Subexpression sharing via DAG construction
- **+3-5% accuracy improvement** (structural pattern recognition)
- **3Ã— fewer edges** in graphs (faster training, lower memory)
- **20-30% memory reduction** during training
- Enabled by default in all configs
- See `SCALED_DATASET.md` for details

### Implemented (Production-Ready)

âœ… **Truth Table Fingerprint** (P0)
- 64-entry truth table for up to 6 variables
- Boolean function signature for equivalence checking

âœ… **Grammar-Constrained Decoding** (P0)
- BNF-based expression grammar
- State machine prevents syntactically invalid outputs

âœ… **3-Tier Verification Cascade** (P0)
- Filters 95% of candidates before expensive Z3 calls
- ~100Ã— speedup vs naive Z3-only verification

âœ… **Copy Mechanism** (P1)
- Pointer-generator network for variable preservation
- Prevents hallucinating non-existent variables

âœ… **Masked Expression Modeling** (P1)
- Self-supervised pretraining task
- Learns expression structure without labels

âœ… **Self-Paced Curriculum Learning** (P1)
- Adaptive difficulty progression
- 4-stage depth curriculum (2â†’5â†’10â†’14)

âœ… **GCNII Over-Smoothing Mitigation**
- Initial residual connections (Î±=0.15)
- Identity mapping with decay (Î»=1.0)
- Prevents over-smoothing in deep GNNs (12+ layers)

âœ… **Operation-Aware Aggregation**
- Commutative ops (ADD, AND, OR, XOR): Sum aggregation
- Non-commutative (SUB): Concatenation + projection
- Preserves mathematical semantics in message passing

âœ… **C++ Fingerprint Acceleration** (Optional)
- 10Ã— speedup via pybind11
- Graceful fallback to pure Python
- Deterministic evaluation

### Optional (Disabled by Default)

ğŸ”§ **Path-Based Edge Encoding**
- Aggregates paths between nodes (up to length 6)
- Enables subexpression sharing recognition
- Flag: `PATH_ENCODING_ENABLED = False`

ğŸ”§ **Global Attention Blocks**
- GraphGPS-style hybrid: local HGT + global self-attention
- Interleaved every 2 layers
- Flag: `HGT_USE_GLOBAL_ATTENTION = False`

### Planned (Not Yet Implemented)

â³ **Process Reward Model** (P3 priority)
- Step-by-step simplification rewards
- Future work for RL improvements

â³ **HTPS Online Learning** (P4 priority)
- Learn new tactics from successful simplifications
- Research phase

---

## Configuration Files

```
configs/
â”œâ”€â”€ phase1.yaml                   # Contrastive pretraining
â”œâ”€â”€ phase2.yaml                   # Supervised learning
â”œâ”€â”€ phase3.yaml                   # RL fine-tuning
â”œâ”€â”€ phase1b_gmn.yaml              # GMN training (frozen encoder)
â”œâ”€â”€ phase1c_gmn_finetune.yaml     # GMN end-to-end fine-tuning
â”œâ”€â”€ scaled_model.yaml             # 360M parameter model
â”œâ”€â”€ semantic_hgt.yaml             # Semantic HGT specific config
â””â”€â”€ example.yaml                  # Reference configuration
```

**Override hyperparameters**:
```bash
python scripts/train.py --phase 2 --config configs/phase2.yaml \
    --learning_rate 3e-4 --batch_size 64
```

---

## Testing

```bash
# Run all tests
pytest tests/ -v

# Specific test modules
pytest tests/test_encoder.py -v
pytest tests/test_data.py -v
pytest tests/test_inference.py -v

# Test coverage
pytest tests/ --cov=src --cov-report=html
```

**Test suite**: 23 test files, ~70% coverage

---

## Known Limitations & Issues

### Active Workarounds

âš ï¸ **Derivative Exclusion** (Low Priority)
- **Issue**: C++ and Python evaluation methods differ for derivatives
- **Impact**: Fingerprint reduced from 448 to 416 dimensions for ML
- **Workaround**: `_strip_derivatives()` in dataset loading
- **Location**: `src/data/dataset.py`, `src/data/fingerprint.py`

âš ï¸ **Placeholder Property Labels** (Medium Priority)
- **Issue**: Property detection using zeros instead of real labels
- **Impact**: Auxiliary loss not fully utilized
- **Status**: Marked `RULE 2 SHOULD_FIX` in `src/training/losses.py`
- **Notes**: Real implementation needs property detector integration

### Performance Characteristics

- **Fingerprint computation**: 1-5ms (C++), 10-50ms (Python)
- **Beam search inference**: 100-500ms per expression
- **HTPS inference**: 500ms-2s per expression
- **Z3 verification**: 100-1000ms per candidate (timeout at 1s)

### Scalability

- **Base model (15M)**: ~60MB weights, 1 GPU sufficient
- **Scaled model (360M)**: ~1.4GB weights, 1-4 GPUs recommended
- **Training time**: Phase 1 (40-60h) + Phase 2 (80-120h) + Phase 3 (20-30h) on single GPU

---

## Dependencies

**Core**:
```
torch >= 2.0.0
torch-geometric >= 2.2.0
numpy >= 1.24.0
z3-solver >= 4.12.0
```

**Optional**:
```
mba_fingerprint_cpp  # C++ acceleration (10Ã— speedup)
wandb                # Experiment tracking
tensorboard          # Training visualization
```

**Install**:
```bash
pip install -r requirements.txt
```

---

## Quick Commands Reference

```bash
# Training
python scripts/train.py --phase 1 --config configs/phase1.yaml
python scripts/train.py --phase 2 --config configs/phase2.yaml --resume checkpoints/phase1_best.pt
python scripts/train.py --phase 3 --config configs/phase3.yaml --resume checkpoints/phase2_best.pt

# Evaluation
python scripts/evaluate.py --checkpoint best.pt --test-set data/test.json

# Inference
python scripts/simplify.py --expr "(x&y)+(x^y)" --checkpoint best.pt --mode beam

# Ablation studies
python scripts/run_ablation.py --encoder gat_jknet --run-id 1
python scripts/run_ablation.py --all-encoders --num-runs 5

# Data generation
python scripts/generate_data.py --depth 1-14 --samples 10M --output data/train.json

# Verification
python scripts/verify_model.py --checkpoint best.pt
python scripts/validate_fingerprint_consistency.py
```

---

## API Quick Reference

### Model Creation
```python
from src.models.full_model import MBADeobfuscator
from src.models.encoder_registry import get_encoder

# Base model
model = MBADeobfuscator(
    encoder_type='gat_jknet',
    hidden_dim=256,
    num_layers=4,
    decoder_layers=6,
    decoder_heads=8
)

# Scaled model
model = MBADeobfuscator(
    encoder_type='hgt',
    hidden_dim=768,
    num_layers=12,
    decoder_layers=8,
    decoder_heads=24,
    decoder_dim=1536
)
```

### Inference
```python
from src.inference.pipeline import InferencePipeline

pipeline = InferencePipeline(model, mode='beam')
result = pipeline.simplify("(x0 & x1) + (x0 ^ x1)")
print(f"Simplified: {result.simplified}")
print(f"Verified: {result.verification_tier == 'z3'}")
```

### Z3 Verification
```python
from src.utils.z3_interface import verify_equivalence

is_equiv, proof = verify_equivalence("(x & y) + (x ^ y)", "x | y")
print(f"Equivalent: {is_equiv}")
```

### Fingerprint Computation
```python
from src.data.fingerprint import SemanticFingerprint

fp = SemanticFingerprint()
vector = fp.compute("(x & y) + (x ^ y)")  # 448-dim
ml_vector = vector[:352] + vector[384:]  # 416-dim (strip derivatives)
```

---

## Documentation

Detailed documentation in `docs/`:

- **ARCHITECTURE.md**: Complete model architecture, dimensions, encoder comparison
- **DATA_PIPELINE.md**: Tokenizer, AST parsing, fingerprinting, batching
- **TRAINING.md**: Training phases, curriculum, loss functions, ablation studies
- **INFERENCE.md**: Beam search, HTPS, verification, reranking details
- **API_REFERENCE.md**: Complete public API documentation

---

## Code Style & Standards

- **Python**: 3.10+
- **Type hints**: Required for all public functions
- **Docstrings**: Google style for public APIs
- **Formatting**: Black (line length 100)
- **Import sorting**: isort
- **Testing**: pytest with ~70% coverage target

---

## Project Status

**Implementation**: 85-90% complete, production-ready

âœ… **Complete**:
- All 8 encoder architectures
- Full data pipeline with C++ acceleration
- 3-phase training + GMN variants
- Inference pipeline with verification
- Z3 integration and utilities

â³ **In Progress**:
- Property detection refinement (placeholder labels â†’ real detection)
- HTPS tactic library expansion (6 â†’ 15-20 tactics)
- Test coverage improvement (70% â†’ 85%)

ğŸ”¬ **Future Work**:
- Process Reward Model (P3)
- HTPS online learning (P4)
- Optional feature tuning (path encoding, global attention)

---

## Citation

```bibtex
@software{mba_deobfuscator,
  title = {MBA Deobfuscator: GNN+Transformer for Polynomial Expression Simplification},
  author = {Your Name},
  year = {2025},
  url = {https://github.com/yourusername/mba-deobfuscator}
}
```

---

## License

[Your license here]

---

**Last Updated**: 2025-01-17 | **Codebase Version**: 0.9.0 (production-ready)
