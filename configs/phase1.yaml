# Phase 1: Contrastive Pretraining Configuration
# Trains encoder using InfoNCE + MaskLM losses

model:
  encoder_type: gat  # gat, ggnn, hgt, rgcn
  hidden_dim: 256
  num_encoder_layers: 4
  num_encoder_heads: 8
  encoder_dropout: 0.1

  # Edge type system: "legacy" (6-type) or "optimized" (8-type)
  # Use "legacy" for backward compatibility with existing datasets
  # Use "optimized" for HGT/RGCN encoders (required) or improved GGNN
  edge_type_mode: legacy

  # Advanced encoder features (experimental)
  use_global_attention: false  # GraphGPS-style hybrid (global + local attention)
  global_attn_interval: 2      # Insert global attention every N layers
  global_attn_heads: 8         # Attention heads for global blocks

  use_path_encoding: false     # Path-based edge encoding for subexpression detection
  path_max_length: 6           # Maximum path length to consider
  path_max_paths: 16           # Maximum paths per edge pair

  operation_aware: false       # Operation-aware aggregation (HGT only)
  operation_aware_strict: true # Raise error if used with non-HGT encoder

  # Decoder (not trained in Phase 1, but needed for model init)
  d_model: 512
  num_decoder_layers: 6
  num_decoder_heads: 8
  d_ff: 2048
  decoder_dropout: 0.1

training:
  # Optimizer
  learning_rate: 0.0001
  weight_decay: 0.01
  warmup_steps: 2000
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
  scheduler_type: cosine  # cosine, linear, constant

  # Training
  batch_size: 64
  num_epochs: 20
  num_workers: 4

  # Phase 1 specific
  infonce_temperature: 0.07
  masklm_mask_ratio: 0.15
  masklm_weight: 0.5

  # Logging
  log_interval: 100
  eval_interval: 1
  save_interval: 5

data:
  train_path: data/train.jsonl
  val_path: data/val.jsonl
  max_depth: null  # No depth filter for contrastive

checkpoint:
  dir: checkpoints/phase1
  resume_from: null  # Path to resume training

logging:
  tensorboard: true
  log_dir: logs/phase1
